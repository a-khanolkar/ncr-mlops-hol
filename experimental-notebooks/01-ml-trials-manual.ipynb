{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.  Data scientist experimentation notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Think of this notebook as a data scientist's scratch notebook where they experiment manually with data preparation, algorithm selection, hyper-paramamet tuning and such.  This notebook does not have any Azure ML Service integration.  The next notebook demonstrates how a data scientist can take this to the next level (integrate with Azure ML) in prep for MLOps build and release pipeline.<br>\n",
    "\n",
    "The experiment is classification type, leveraging ScikitLearn library.  Use case is Coronary Heart Disease Prediction with the famous Kaggle dataset - framingham.csv<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load necessary packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectFromModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rewmove local copy iof file\n",
    "if os.path.exists(\"framingham.csv\"):\n",
    "  os.remove(\"framingham.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-01-24 00:22:15--  https://mlopssa.blob.core.windows.net/chd-dataset/framingham.csv\n",
      "Resolving mlopssa.blob.core.windows.net (mlopssa.blob.core.windows.net)... 52.225.136.36\n",
      "Connecting to mlopssa.blob.core.windows.net (mlopssa.blob.core.windows.net)|52.225.136.36|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 191805 (187K) [text/csv]\n",
      "Saving to: ‘framingham.csv.1’\n",
      "\n",
      "framingham.csv.1    100%[===================>] 187.31K  --.-KB/s    in 0.003s  \n",
      "\n",
      "2020-01-24 00:22:15 (65.1 MB/s) - ‘framingham.csv.1’ saved [191805/191805]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Download load file from YOUR storage account URL\n",
    "!wget \"https://mlopssa.blob.core.windows.net/chd-dataset/framingham.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read into a Pandas dataframe\n",
    "df = pd.read_csv('framingham.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a boolean array of smokers\n",
    "smoke = (df['currentSmoker']==1)\n",
    "# Apply mean to NaNs in cigsPerDay but using a set of smokers only\n",
    "df.loc[smoke,'cigsPerDay'] = df.loc[smoke,'cigsPerDay'].fillna(df.loc[smoke,'cigsPerDay'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill out missing values\n",
    "df['BPMeds'].fillna(0, inplace = True)\n",
    "df['glucose'].fillna(df.glucose.mean(), inplace = True)\n",
    "df['totChol'].fillna(df.totChol.mean(), inplace = True)\n",
    "df['education'].fillna(1, inplace = True)\n",
    "df['BMI'].fillna(df.BMI.mean(), inplace = True)\n",
    "df['heartRate'].fillna(df.heartRate.mean(), inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df.iloc[:,:-1]\n",
    "result = df.iloc[:,-1] # the last column is what we are about to forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(features, result, test_size = 0.2, random_state = 14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_estimators=100, max_depth=2, random_state=0)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a selector object that will use the random forest classifier to identify\n",
    "# features that have an importance of more than 0.12\n",
    "sfm = SelectFromModel(clf, threshold=0.12)\n",
    "\n",
    "# Train the selector\n",
    "sfm.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_labels = list(features.columns.values) # creating a list with features' names\n",
    "for feature_list_index in sfm.get_support(indices=True):\n",
    "    print(feat_labels[feature_list_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = clf.feature_importances_\n",
    "std = np.std([tree.feature_importances_ for tree in clf.estimators_],\n",
    "             axis=0)\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "print(\"Feature ranking:\")\n",
    "for f in range(X_train.shape[1]):\n",
    "    print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With only imporant features. Can check X_important_train.shape[1]\n",
    "X_important_train = sfm.transform(X_train)\n",
    "X_important_test = sfm.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_important = RandomForestClassifier(n_estimators=10000, random_state=0, n_jobs=-1)\n",
    "clf_important.fit(X_important_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The metrics are..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_y_4 = clf_important.predict(X_important_test)\n",
    "\n",
    "print(\"============================\")\n",
    "print(\"Classification Report\")\n",
    "print(\"============================\")\n",
    "print(classification_report(y_test, predictions_y_4))\n",
    "print(\"\")\n",
    "\n",
    "print(\"============================\")\n",
    "print(\"Confusion Matrix\")\n",
    "print(\"============================\")\n",
    "print(confusion_matrix(y_test, predictions_y_4))\n",
    "print(\"\")\n",
    "\n",
    "# Under ROC curve\n",
    "print(\"============================\")\n",
    "print(\"ROC\")\n",
    "print(\"============================\")\n",
    "prob_y_4 = clf_important.predict_proba(X_important_test)\n",
    "prob_y_4 = [p[1] for p in prob_y_4]\n",
    "print(roc_auc_score(y_test, prob_y_4))\n",
    "print(\"\")\n",
    "\n",
    "print(\"============================\")\n",
    "print(\"Accuracy score\")\n",
    "print(\"============================\")\n",
    "accuracy_score(y_test, predictions_y_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('./outputs/model', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model path on disk\n",
    "filename = './outputs/model/chd-rf-model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model to disk\n",
    "pickle.dump(clf_important, open(filename, 'wb'))\n",
    "print(\"model saved in ././outputs/model/ folder\")\n",
    "print(\"Saving model files completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n"
     ]
    }
   ],
   "source": [
    "# Lets run a prediction\n",
    "# Test... Actual: TenYearCHD=1\n",
    "\n",
    "age = 61\n",
    "prevalentHyp = 1\n",
    "sysBP = 150\n",
    "glucose = 103\n",
    " \n",
    "# load the model from disk\n",
    "loaded_model = pickle.load(open(filename, 'rb'))\n",
    "print(loaded_model.predict([[age, prevalentHyp, sysBP, glucose]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'loaded_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-da2d1eff7c7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mglucose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m99\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloaded_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprevalentHyp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msysBP\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglucose\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'loaded_model' is not defined"
     ]
    }
   ],
   "source": [
    "# Lets run a prediction\n",
    "# Test... Actual: TenYearCHD=0\n",
    "\n",
    "age = 43\n",
    "prevalentHyp = 1\n",
    "sysBP = 180\n",
    "glucose = 99\n",
    " \n",
    "print(loaded_model.predict([[age, prevalentHyp, sysBP, glucose]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n"
     ]
    }
   ],
   "source": [
    "# Lets run a prediction\n",
    "# Test... Actual: TenYearCHD=1\n",
    "\n",
    "age = 63\n",
    "prevalentHyp = 0\n",
    "sysBP = 138\n",
    "glucose = 85\n",
    " \n",
    "print(loaded_model.predict([[age, prevalentHyp, sysBP, glucose]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n"
     ]
    }
   ],
   "source": [
    "# Lets run a prediction\n",
    "# Test... Actual: TenYearCHD=0\n",
    "age = 52\n",
    "prevalentHyp = 1\n",
    "sysBP = 141\n",
    "glucose = 75\n",
    " \n",
    "print(loaded_model.predict([[age, prevalentHyp, sysBP, glucose]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets run multiple predictions\n",
    "results = loaded_model.predict([[61, 1, 150, 103],[43, 1, 180, 99],[63,0,138,85]])\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "# Lets try in the format in the way the REST service we operationalize the model to, expects it\n",
    "import json\n",
    "\n",
    "to_be_scored_json = {\"data\":[[61, 1, 150, 103],[43, 1, 180, 99],[63,0,138,85]]}\n",
    "input_data_json = json.dumps(to_be_scored_json)\n",
    "\n",
    "to_be_scored_list = json.loads(input_data_json)[\"data\"]\n",
    "to_be_scored_list=np.array(to_be_scored_list)\n",
    "print(loaded_model.predict(to_be_scored_list).tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next\n",
    "Return to the lab guide for the next step."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
